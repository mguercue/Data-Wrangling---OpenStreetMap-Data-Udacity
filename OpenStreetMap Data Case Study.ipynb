{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Case Study\n",
    "\n",
    "### by Murat Guercue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Area\n",
    "\n",
    "Orlando / Florida, USA.\n",
    "\n",
    "•http://www.openstreetmap.org/relation/1128379#map=12/28.4815/-81.3675\n",
    "\n",
    "This map is of my favorite city Orlando (USA/Florida), so I'd like to explore open-source map of this area, reveal some inconsistencies in data and contribute to its improvement on OpenStreetMap.org.\n",
    "\n",
    "As described in the introductions, my first action is to split the Orldons.osm File into a sample file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"Openstreetmap_Orlando.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 30 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/\n",
    "    inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "    # changed code for Python 3\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write(bytes('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n', 'UTF-8'))\n",
    "    output.write(bytes('<osm>\\n  ', 'UTF-8'))\n",
    "\n",
    "    # Write every 10th top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % 10 == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write(bytes('</osm>', 'UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems entcountered\n",
    "\n",
    "## Data Auditing\n",
    "\n",
    "Our first step is to get an overview of our dataset, to be able to audit the Orlando_osm file. Therefore, we count the tag content of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 23954,\n",
      " 'meta': 1,\n",
      " 'nd': 331444,\n",
      " 'node': 270522,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 663,\n",
      " 'tag': 119477,\n",
      " 'way': 26206}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET \n",
    "import pprint\n",
    " \n",
    "OSM_FILE = \"Openstreetmap_Orlando.osm\" \n",
    " \n",
    "def count_tags(filename): \n",
    "    tags = {} \n",
    "    for event, elem in ET.iterparse(filename): \n",
    "        if elem.tag in tags:  \n",
    "            tags[elem.tag] += 1\n",
    "        else: \n",
    "            tags[elem.tag] = 1\n",
    "    return tags \n",
    "            \n",
    "pprint.pprint(count_tags(OSM_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we can see our Orlando_osm dataset has following tags:\n",
    "\n",
    "    - member: 23954\n",
    "    - meta: 1\n",
    "    - nd: 331444\n",
    "    - node: 270522\n",
    "    - note: 1\n",
    "    - osm: 1\n",
    "    - relation: 663\n",
    "    - tag: 119477\n",
    "    - way: 26206"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of our \"k\" values are different. Now we will check what is the structure of \"k\" value and which are valid or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 81188, 'lower_colon': 33527, 'other': 4762, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET \n",
    "import pprint \n",
    "import re \n",
    " \n",
    "from collections import defaultdict \n",
    " \n",
    "lower = re.compile(r'^([a-z]|_)*$') \n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$') \n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]') \n",
    "\n",
    " \n",
    "OSM_FILE = \"Openstreetmap_Orlando.osm\" \n",
    "\n",
    " \n",
    "def key_type(element, keys): \n",
    "    if element.tag == \"tag\": \n",
    "        for tag in element.iter('tag'): \n",
    "            k = tag.get('k') \n",
    "            if lower.search(element.attrib['k']): \n",
    "                keys['lower'] = keys['lower'] + 1 \n",
    "            elif lower_colon.search(element.attrib['k']): \n",
    "                keys['lower_colon'] = keys['lower_colon'] + 1 \n",
    "            elif problemchars.search(element.attrib['k']): \n",
    "                keys['problemchars'] = keys['problemchars'] + 1 \n",
    "            else: \n",
    "                keys['other'] = keys['other'] + 1 \n",
    "      \n",
    "    return keys \n",
    "\n",
    "def process_map(filename): \n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0} \n",
    "    for _, element in ET.iterparse(filename): \n",
    "        keys = key_type(element, keys) \n",
    " \n",
    "    return keys \n",
    "\n",
    "pprint.pprint(process_map(OSM_FILE)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify the follwing structure at our Orlando_osm dataset.\n",
    "\n",
    "\n",
    "• \"lower\" : 81188 , for valid tags with only lowercase letters,\n",
    "\n",
    "• \"lower_colon\" : 33527 , for tags with a colon which are also valid,\n",
    "\n",
    "• \"problemchars\" : 0 , for tags with special/problematic characters, and\n",
    "\n",
    "• \"other\" : 4762 , for other which are outside of the other groups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will focus on problems of our dataset. If we check the Street Names we can assess different structures. We can see some different abbreviations and different writing structures. No we will harmonize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSM_FILE = \"Openstreetmap_Orlando.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Highway\"]\n",
    "\n",
    "# Abbreviations\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Hwy\": \"Highway\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Rd.\": \"Road\"\n",
    "            }\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if m not in expected:\n",
    "        name = re.sub(m.group(), mapping[m.group()], name)\n",
    "    \n",
    "    num_line_street_re = re.compile(r'\\d0?(st|nd|rd|th|)\\s(Line)$', re.IGNORECASE)\n",
    "    num_line_mapping = {\n",
    "                         \"13th\": \"Thirteen\",\n",
    "                         \"19th\": \"Nineteen\"\n",
    "                       }\n",
    "    \n",
    "    if num_line_street_re.match(name):\n",
    "        nth = nth_re.search(name)\n",
    "        name = num_line_mapping[nth.group(0)] + \" Line\"\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSM_FILE)\n",
    "    assert len(st_types) == 3\n",
    "    pprint.pprint(dict(st_types))\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After converting the Street Names we will now focus on phone numbers and harmonize them to the structure +1 XXX XXX XXXX. Our origin dataset different structures with brackets, with minus or without any space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_phone_num(phone_num):\n",
    "    # Check for valid phone number format\n",
    "    p = PHONENUM.match(phone_num)\n",
    "    if p is None:\n",
    "        # Remove all brackets\n",
    "        if \"(\" in phone_num or \")\" in phone_num:\n",
    "            phone_num = re.sub(\"[()]\", \"\", phone_num)\n",
    "        # Convert all dashes to spaces\n",
    "        if \"-\" in phone_num:\n",
    "            phone_num = re.sub(\"-\", \" \", phone_num)\n",
    "        # Space out 10 straight numbers\n",
    "        if re.match(r'\\d{10}', phone_num) is not None:\n",
    "            phone_num = phone_num[:3] + \" \" + phone_num[3:6] + \" \" + phone_num[6:]\n",
    "        # Space out 11 straight numbers\n",
    "        elif re.match(r'\\d{11}', phone_num) is not None:\n",
    "            phone_num = phone_num[:1] + \" \" + phone_num[1:4] + \" \" + phone_num[4:7] \\\n",
    "            \t\t\t+ \" \" + phone_num[7:]\n",
    "        # Add full country code\n",
    "        if re.match(r'\\d{3}\\s\\d{3}\\s\\d{4}', phone_num) is not None:\n",
    "            phone_num = \"+1 \" + phone_num\n",
    "        # Add + in country code\n",
    "        elif re.match(r'1\\s\\d{3}\\s\\d{3}\\s\\d{4}', phone_num) is not None:\n",
    "            phone_num = \"+\" + phone_num\n",
    "        # Ignore tag if no area code and local number (<10 digits)\n",
    "        elif sum(c.isdigit() for c in phone_num) < 10:\n",
    "            return None\n",
    "    return phone_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Overview\n",
    "\n",
    "Now we will generate some general information about our dataset.\n",
    "\n",
    "First step is to prepare the dataset to be inserted into a SQL database.\n",
    "To do so we will parse the elements in the OSM XML file, transforming them from document format to\n",
    "tabular format, thus making it possible to write to .csv files.  These csv files can then easily be\n",
    "imported to a SQL database as tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'cerberus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-709ae43fbc62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0munittest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTestCase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcerberus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjsonschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'cerberus'"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import codecs \n",
    "import re \n",
    "import xml.etree.cElementTree as ET \n",
    "from unittest import TestCase \n",
    " \n",
    "import cerberus \n",
    "import jsonschema \n",
    " \n",
    "OSM_PATH = \"Openstreetmap_Orlando.osm\" \n",
    " \n",
    "NODES_PATH = \"nodes.csv\" \n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\" \n",
    "WAYS_PATH = \"ways.csv\" \n",
    "WAY_NODES_PATH = \"ways_nodes.csv\" \n",
    "WAY_TAGS_PATH = \"ways_tags.csv\" \n",
    " \n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+') \n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]') \n",
    " \n",
    "SCHEMA = jsonschema.jsonschema \n",
    " \n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp'] \n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type'] \n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp'] \n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type'] \n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position'] \n",
    " \n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS, \n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'): \n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\" \n",
    " \n",
    "    node_attribs = {} \n",
    "    way_attribs = {} \n",
    "    way_nodes = [] \n",
    "    tags = []   \n",
    "\n",
    "    if element.tag == 'node': \n",
    "        for attrib in element.attrib: \n",
    "            if attrib in NODE_FIELDS: \n",
    "                node_attribs[attrib] = element.attrib[attrib] \n",
    "       \n",
    "        for child in element: \n",
    "            node_tag = {} \n",
    "            if LOWER_COLON.match(child.attrib['k']): \n",
    "                node_tag['type'] = child.attrib['k'].split(':',1)[0] \n",
    "                node_tag['key'] = child.attrib['k'].split(':',1)[1] \n",
    "                node_tag['id'] = element.attrib['id'] \n",
    "                node_tag['value'] = child.attrib['v'] \n",
    "                tags.append(node_tag) \n",
    "            elif PROBLEMCHARS.match(child.attrib['k']): \n",
    "                continue \n",
    "            else: \n",
    "                node_tag['type'] = 'regular' \n",
    "                node_tag['key'] = child.attrib['k'] \n",
    "                node_tag['id'] = element.attrib['id'] \n",
    "                node_tag['value'] = child.attrib['v'] \n",
    "                tags.append(node_tag) \n",
    "        \n",
    "        return {'node': node_attribs, 'node_tags': tags} \n",
    "         \n",
    "    elif element.tag == 'way': \n",
    "        for attrib in element.attrib: \n",
    "            if attrib in WAY_FIELDS: \n",
    "                way_attribs[attrib] = element.attrib[attrib] \n",
    "        \n",
    "        position = 0 \n",
    "        for child in element: \n",
    "            way_tag = {} \n",
    "            way_node = {} \n",
    "            \n",
    "            if child.tag == 'tag': \n",
    "                if LOWER_COLON.match(child.attrib['k']): \n",
    "                    way_tag['type'] = child.attrib['k'].split(':',1)[0] \n",
    "                    way_tag['key'] = child.attrib['k'].split(':',1)[1] \n",
    "                    way_tag['id'] = element.attrib['id'] \n",
    "                    way_tag['value'] = child.attrib['v'] \n",
    "                    tags.append(way_tag) \n",
    "                elif PROBLEMCHARS.match(child.attrib['k']): \n",
    "                    continue \n",
    "                else: \n",
    "                    way_tag['type'] = 'regular' \n",
    "                    way_tag['key'] = child.attrib['k'] \n",
    "                    way_tag['id'] = element.attrib['id'] \n",
    "                    way_tag['value'] = child.attrib['v'] \n",
    "                    tags.append(way_tag) \n",
    "                     \n",
    "            elif child.tag == 'nd': \n",
    "                way_node['id'] = element.attrib['id'] \n",
    "                way_node['node_id'] = child.attrib['ref'] \n",
    "                way_node['position'] = position \n",
    "                position += 1 \n",
    "                way_nodes.append(way_node) \n",
    "       \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags} \n",
    "\n",
    "# ================================================== # \n",
    "#               Helper Functions                     # \n",
    "# ================================================== # \n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')): \n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\" \n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end')) \n",
    "    _, root = next(context) \n",
    "    for event, elem in context: \n",
    "        if event == 'end' and elem.tag in tags: \n",
    "            yield elem \n",
    "            root.clear() \n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA): \n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\" \n",
    "    if validator.validate(element, schema) is not True: \n",
    "        field, errors = next(validator.errors.iteritems()) \n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\" \n",
    "        error_strings = ( \n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v)) \n",
    "            for k, v in errors.iteritems() \n",
    "        ) \n",
    "        raise cerberus.ValidationError( \n",
    "            message_string.format(field, \"\\n\".join(error_strings)) \n",
    "        ) \n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object): \n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\" \n",
    "\n",
    "    def writerow(self, row): \n",
    "        super(UnicodeDictWriter, self).writerow({ \n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems() \n",
    "        }) \n",
    "\n",
    "    def writerows(self, rows): \n",
    "        for row in rows: \n",
    "            self.writerow(row) \n",
    " \n",
    " \n",
    "# ================================================== # \n",
    "#               Main Function                        # \n",
    "# ================================================== # \n",
    "def process_map(file_in, validate): \n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\" \n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file,\\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file,\\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file,\\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file,\\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file: \n",
    " \n",
    " \n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS) \n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS) \n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS) \n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS) \n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS) \n",
    "\n",
    "        nodes_writer.writeheader() \n",
    "        node_tags_writer.writeheader() \n",
    "        ways_writer.writeheader() \n",
    "        way_nodes_writer.writeheader() \n",
    "        way_tags_writer.writeheader() \n",
    " \n",
    "        validator = cerberus.Validator() \n",
    " \n",
    "        for element in get_element(file_in, tags=('node', 'way')): \n",
    "            el = shape_element(element) \n",
    "            if el: \n",
    "                if validate is True: \n",
    "                    validate_element(el, validator) \n",
    "\n",
    " \n",
    "                if element.tag == 'node': \n",
    "                    nodes_writer.writerow(el['node']) \n",
    "                    node_tags_writer.writerows(el['node_tags']) \n",
    "                elif element.tag == 'way': \n",
    "                    ways_writer.writerow(el['way']) \n",
    "                    way_nodes_writer.writerows(el['way_nodes']) \n",
    "                    way_tags_writer.writerows(el['way_tags']) \n",
    "\n",
    "if __name__ == '__main__': \n",
    "    process_map(OSM_PATH, validate=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After perparing our dataset and creating the csv files we will investigate now our dataset and provide some general statistics by using SQL queries.\n",
    "\n",
    "Firt we start with file sizes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let´s focus on number of Ways and Nodes.\n",
    "\n",
    "Ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> _sqlite> SELECT COUNT(*) FROM Ways;_ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXXX\n",
    "\n",
    "Nodes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> _sqlite> SELECT COUNT(*) FROM Nodes;_ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXXX\n",
    "\n",
    "Top 20 Contributers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> sqlite> SELECT e.user, COUNT(*) as num\n",
    "FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e\n",
    "GROUP BY e.user\n",
    "ORDER BY num DESC\n",
    "LIMIT 20; </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Additional Data Exploration\n",
    "\n",
    "Top 5 Cuisines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> sqlite> SELECT nodesTags.value, COUNT(*) as num\n",
    "\n",
    "FROM nodesTags\n",
    "\n",
    "JOIN (SELECT DISTINCT(id) FROM nodesTags WHERE value='restaurant') i\n",
    "\n",
    "ON nodesTags.id=i.id\n",
    "\n",
    "WHERE nodesTags.key='cuisine'    \n",
    "\n",
    "GROUP BY nodesTags.value\n",
    "\n",
    "ORDER BY num DESC\n",
    "\n",
    "LIMIT 5; </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "XXXX\n",
    "\n",
    "Biggest Religion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> sqlite> SELECT nodes_tags.value, COUNT(*) as num\n",
    "FROM nodes_tags \n",
    "    JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='place_of_worship') i\n",
    "    ON nodes_tags.id=i.id\n",
    "WHERE nodes_tags.key='religion'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY num DESC\n",
    "LIMIT 1; </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Our Orlando dataset is despite of huge size in general of good quality but showing some typical differences/errors which needs harmonization and adjustments. By huge number of Users contributing to the OpenStreetMaps it´s very likely to figure out human made typo errors. For typo errors like street name abbreviations and different phone number structures we used codes learned at our lessons to harmonize and adjust them.\n",
    "\n",
    "After parsing our dataset we could also find some interesting statistics of Orlando.\n",
    "\n",
    "In general our dataset is very useful, but needs some adjustments in some areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
